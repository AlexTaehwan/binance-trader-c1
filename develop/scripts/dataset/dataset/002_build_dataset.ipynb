{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from common_utils import make_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'rawdata_dir': '../../../storage/dataset/rawdata/csv/',\n",
    "    'data_store_dir': '../../../storage/dataset/dataset_10m/',\n",
    "    \"winsorize_threshold\": 0.6,\n",
    "    \"lookahead_window\": 10,\n",
    "    'train_ratio': 0.7,\n",
    "}\n",
    "COLUMNS = ['open', 'high', 'low', 'close']\n",
    "\n",
    "\n",
    "def load_rawdata(file_name):\n",
    "    rawdata = pd.read_csv(file_name, header=0, index_col=0)[COLUMNS]\n",
    "    rawdata.index = pd.to_datetime(rawdata.index)\n",
    "    \n",
    "    return rawdata\n",
    "\n",
    "def _build_feature_by_rawdata(rawdata):\n",
    "    returns = rawdata.pct_change(1, fill_method=None).iloc[1:].rename(\n",
    "        columns={\n",
    "            key: key + '_return'\n",
    "            for key in COLUMNS\n",
    "        }\n",
    "    )\n",
    "\n",
    "    inner_changes = []\n",
    "    for column_pair in sorted(list(combinations(COLUMNS, 2))):\n",
    "        inner_changes.append(rawdata[list(column_pair)].pct_change(1, axis=1, fill_method=None)[column_pair[-1]].rename('_'.join(column_pair) + '_change'))\n",
    "\n",
    "    inner_changes = pd.concat(inner_changes, axis=1).reindex(returns.index)\n",
    "\n",
    "    return pd.concat([returns, inner_changes], axis=1).sort_index()\n",
    "\n",
    "def build_features(file_names):\n",
    "    features = []\n",
    "    for file_name in tqdm(file_names):\n",
    "        coin_pair = file_name.split('/')[-1].split('.')[0]\n",
    "\n",
    "        rawdata = load_rawdata(file_name=file_name)\n",
    "        feature = _build_feature_by_rawdata(rawdata=rawdata)\n",
    "        feature.columns = pd.MultiIndex.from_tuples(sorted([(coin_pair, column) for column in feature.columns]))\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return pd.concat(features, axis=1).dropna()\n",
    "\n",
    "def _build_fwd_returns_by_rawdata(rawdata, lookahead_window):\n",
    "    fwd_returns = []\n",
    "    for column_pair in [('close', 'high'), ('close', 'low')]:\n",
    "        partial_fwd_returns = []\n",
    "        for window in range(1, lookahead_window + 1):\n",
    "            colum_pair_df = rawdata[list(column_pair)].copy().sort_index()\n",
    "            colum_pair_df.columns = [0, 1]\n",
    "\n",
    "            colum_pair_df[1] = colum_pair_df[1].shift(-window)\n",
    "            partial_fwd_return = colum_pair_df.pct_change(1, axis=1, fill_method=None)[1].rename(f'fwd_return({window})')        \n",
    "            partial_fwd_returns.append(partial_fwd_return)\n",
    "\n",
    "        partial_fwd_returns = pd.concat(partial_fwd_returns, axis=1).sort_index()\n",
    "        partial_fwd_returns.columns = ['_'.join(column_pair) + '_' + column for column in partial_fwd_returns.columns]\n",
    "        fwd_returns.append(partial_fwd_returns)\n",
    "\n",
    "    return pd.concat(fwd_returns, axis=1).sort_index()\n",
    "\n",
    "def build_fwd_returns(file_names, lookahead_window):\n",
    "    total_fwd_returns = []\n",
    "    for file_name in tqdm(file_names):\n",
    "        coin_pair = file_name.split('/')[-1].split('.')[0]\n",
    "\n",
    "        rawdata = load_rawdata(file_name=file_name)\n",
    "        fwd_returns = _build_fwd_returns_by_rawdata(rawdata=rawdata, lookahead_window=lookahead_window)\n",
    "        fwd_returns.columns = pd.MultiIndex.from_tuples(sorted([(coin_pair, column) for column in fwd_returns.columns]))\n",
    "\n",
    "        total_fwd_returns.append(fwd_returns)\n",
    "\n",
    "    return pd.concat(total_fwd_returns, axis=1).dropna()\n",
    "\n",
    "def build_scaler(features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features)\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def preprocess_features(features, scaler, winsorize_threshold):\n",
    "    index = features.index\n",
    "    columns = features.columns\n",
    "\n",
    "    processed_features = pd.DataFrame(scaler.transform(features), index=index, columns=columns)\n",
    "    \n",
    "    # winsorize\n",
    "    return processed_features.clip(-winsorize_threshold, winsorize_threshold)\n",
    "\n",
    "def main(\n",
    "    rawdata_dir=CONFIG['rawdata_dir'],\n",
    "    data_store_dir=CONFIG['data_store_dir'],\n",
    "    winsorize_threshold=CONFIG[\"winsorize_threshold\"],\n",
    "    lookahead_window=CONFIG['lookahead_window'],\n",
    "):\n",
    "    # Make dirs\n",
    "    train_data_store_dir = os.path.join(data_store_dir, 'train')\n",
    "    test_data_store_dir = os.path.join(data_store_dir, 'test')\n",
    "    make_dirs([data_store_dir, scaler_store_dir, train_data_store_dir, test_data_store_dir])\n",
    "\n",
    "    # Build features\n",
    "    file_names = glob(os.path.join(rawdata_dir, '*'))\n",
    "    features = build_features(file_names)\n",
    "    scaler = build_scaler(features)\n",
    "\n",
    "    features = preprocess_features(features=features, scaler=scaler, winsorize_threshold=winsorize_threshold)\n",
    "\n",
    "    # Store Artifacts\n",
    "    boundary_index = int(len(features.index) * CONFIG['train_ratio'])\n",
    "    features.iloc[:boundary_index].to_csv(\n",
    "        os.path.join(train_data_store_dir, 'X.csv'))\n",
    "    features.iloc[boundary_index:].to_csv(\n",
    "        os.path.join(test_data_store_dir, 'X.csv'))\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(CONFIG['data_store_dir'], 'scaler.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_window=CONFIG['lookahead_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:41<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "file_names = glob(os.path.join(CONFIG['rawdata_dir'], '*'))\n",
    "fwd_returns = build_fwd_returns(file_names, lookahead_window=lookahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
